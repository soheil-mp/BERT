{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwFLnqwbDPbl"
      },
      "source": [
        "# End-to-end Masked Language Modeling with BERT\n",
        "\n",
        "**Author:** [Ankur Singh](https://twitter.com/ankur310794)<br>\n",
        "**Date created:** 2020/09/18<br>\n",
        "**Last modified:** 2020/09/18<br>\n",
        "**Description:** Implement a Masked Language Model (MLM) with BERT and fine-tune it on the IMDB Reviews dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Vv8gZFDPbq"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Masked Language Modeling is a fill-in-the-blank task,\n",
        "where a model uses the context words surrounding a mask token to try to predict what the\n",
        "masked word should be.\n",
        "\n",
        "For an input that contains one or more mask tokens,\n",
        "the model will generate the most likely substitution for each.\n",
        "\n",
        "Example:\n",
        "\n",
        "- Input: \"I have watched this [MASK] and it was awesome.\"\n",
        "- Output: \"I have watched this movie and it was awesome.\"\n",
        "\n",
        "Masked language modeling is a great way to train a language\n",
        "model in a self-supervised setting (without human-annotated labels).\n",
        "Such a model can then be fine-tuned to accomplish various supervised\n",
        "NLP tasks.\n",
        "\n",
        "This example teaches you how to build a BERT model from scratch,\n",
        "train it with the masked language modeling task,\n",
        "and then fine-tune this model on a sentiment classification task.\n",
        "\n",
        "We will use the Keras `TextVectorization` and `MultiHeadAttention` layers\n",
        "to create a BERT Transformer-Encoder network architecture.\n",
        "\n",
        "Note: This example should be run with `tf-nightly`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zQSDmzrgDPbr"
      },
      "source": [
        "<br>\n",
        "\n",
        "### INITIAL SETUP\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLtZuMe-DPbt"
      },
      "outputs": [],
      "source": [
        "# Import the libraires\n",
        "import pprint, glob, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmGUeDSeDPby"
      },
      "outputs": [],
      "source": [
        "# Configuration class\n",
        "@dataclass\n",
        "class Config:\n",
        "    \n",
        "    # Parameters\n",
        "    MAX_LEN = 256         # Maximum length of input sentence to the model.\n",
        "    BATCH_SIZE = 32       # Batch size for training.\n",
        "    LR = 0.001            # Learning rate for Adam optimizer.\n",
        "    VOCAB_SIZE = 30000    # Size of the vocabulary.\n",
        "    EMBED_DIM = 128       # Embedding size for embedding matrix\n",
        "    NUM_HEAD = 8          # Number of attention heads\n",
        "    FF_DIM = 128          # Hidden layer size in feed forward network inside transformer\n",
        "    NUM_LAYERS = 1        # Number of transformer layers\n",
        "\n",
        "# Initialize the configuration\n",
        "config = Config()\n",
        "config"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nVP_0MI2DPby"
      },
      "source": [
        "<br>\n",
        "\n",
        "### DOWNLOAD AND LOAD DATASET\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1Ifhmh-DPbz"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for getting the list of files\n",
        "def get_text_list_from_files(files):\n",
        "    \n",
        "    # Initialize a list\n",
        "    text_list = []\n",
        "    \n",
        "    # Loop over the files\n",
        "    for name in files:\n",
        "        \n",
        "        # Open the file\n",
        "        with open(name) as f:\n",
        "            \n",
        "            # Loop over the lines\n",
        "            for line in f:\n",
        "                \n",
        "                # Append the line to the list\n",
        "                text_list.append(line)\n",
        "                \n",
        "    return text_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for getting the data from the text files\n",
        "def get_data_from_text_files(folder_name):\n",
        "\n",
        "    # List of files for positive reviews\n",
        "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
        "    \n",
        "    # Get the list of texts from the file\n",
        "    pos_texts = get_text_list_from_files(pos_files)\n",
        "    \n",
        "    # List of files for negative reviews\n",
        "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
        "    \n",
        "    # Get the list of texts from the file\n",
        "    neg_texts = get_text_list_from_files(neg_files)\n",
        "    \n",
        "    # Add the positive and negative reviews to a dataframe\n",
        "    df = pd.DataFrame({\"review\": pos_texts + neg_texts, \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),})\n",
        "    \n",
        "    # Shuffle the dataframe\n",
        "    df = df.sample(len(df)).reset_index(drop=True)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the training and testing dataset\n",
        "train_df = get_data_from_text_files(\"train\")\n",
        "test_df = get_data_from_text_files(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine the training and testing dataset\n",
        "all_data = train_df.append(test_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u-kZLYjaDPcF"
      },
      "source": [
        "<br>\n",
        "\n",
        "### DATA PREPARATION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for custom standardization\n",
        "def custom_standardization(input_data):\n",
        "    \n",
        "    # Lowercase the data\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    \n",
        "    # Remove the html tags\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    \n",
        "    # Remove the punctuations\n",
        "    stripped_punc = tf.strings.regex_replace(stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\")\n",
        "    \n",
        "    return stripped_punc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for getting the vectorize layer\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "\n",
        "    # Vectorize the text\n",
        "    vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size,\n",
        "                                                        output_mode=\"int\",\n",
        "                                                        standardize=custom_standardization,\n",
        "                                                        output_sequence_length=max_seq,)\n",
        "    \n",
        "    # Apply to the text\n",
        "    vectorize_layer.adapt(texts)\n",
        "\n",
        "    # Get the vocabulary\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    \n",
        "    # Insert mask token in vocabulary\n",
        "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
        "    \n",
        "    # Update the vocabulary\n",
        "    vectorize_layer.set_vocabulary(vocab)\n",
        "    \n",
        "    return vectorize_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the vectorize layer\n",
        "vectorize_layer = get_vectorize_layer(all_data.review.values.tolist(),\n",
        "                                      config.VOCAB_SIZE,\n",
        "                                      config.MAX_LEN,\n",
        "                                      special_tokens=[\"[mask]\"],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mask token id\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for encoding the text\n",
        "def encode(texts):\n",
        "    \n",
        "    # Vextorize the text\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    \n",
        "    # Convert to numpy\n",
        "    encoded_texts = encoded_texts.numpy()\n",
        "    \n",
        "    return encoded_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for getting the masked input and labels\n",
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    \n",
        "    # 15% BERT masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "    \n",
        "    # Do not mask special tokens\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    \n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    \n",
        "    # Set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    # Prepare input\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    \n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens. This means leaving 10% unchanged.\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[inp_mask_2mask] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "    # Set 10% to a random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(3, mask_token_id, inp_mask_2random.sum())\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "\n",
        "    return encoded_texts_masked, y_labels, sample_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode the inupt training data \n",
        "x_train = encode(train_df.review.values)  \n",
        "\n",
        "# Get the output training data\n",
        "y_train = train_df.sentiment.values\n",
        "\n",
        "# Convert to tf.data + Shuffle + Set batch size\n",
        "train_classifier_ds = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(config.BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode the input testing data\n",
        "x_test = encode(test_df.review.values)\n",
        "\n",
        "# Get the output testing data\n",
        "y_test = test_df.sentiment.values\n",
        "\n",
        "# Convert to tf.data + Set batch size\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(config.BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the raw test set to tf.data + Set batch size\n",
        "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((test_df.review.values, y_test)).batch(config.BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for masked language model\n",
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert masked data to tf.data\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_train, y_masked_labels, sample_weights))\n",
        "\n",
        "# Shuffle + Set batch size\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oT4EqYcYDPcG"
      },
      "source": [
        "<br>\n",
        "\n",
        "### BERT MODEL\n",
        "\n",
        "Create BERT model (Pretraining Model) for masked language modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementation of the BERT module\n",
        "def bert_module(query, key, value, i):\n",
        "    \n",
        "    # Multi headed self-attention\n",
        "    attention_output = tf.keras.layers.MultiHeadAttention(num_heads=config.NUM_HEAD,\n",
        "                                                          key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "                                                          name=\"encoder_{}/multiheadattention\".format(i),\n",
        "                                                         )(query, key, value)\n",
        "    \n",
        "    # Dropout   \n",
        "    attention_output = tf.keras.layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(attention_output)\n",
        "    \n",
        "    # Add and norm\n",
        "    attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
        "                                                         )(query + attention_output)\n",
        "\n",
        "    # Initialize dense layers\n",
        "    ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(config.EMBED_DIM),\n",
        "        ],\n",
        "        name=\"encoder_{}/ffn\".format(i),\n",
        "    )\n",
        "    \n",
        "    # Feed-forward\n",
        "    ffn_output = ffn(attention_output)\n",
        "    \n",
        "    # Dropout\n",
        "    ffn_output = tf.keras.layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(ffn_output)\n",
        "    \n",
        "    # Add and norm\n",
        "    sequence_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
        "                                                        )(attention_output + ffn_output)\n",
        "    \n",
        "    return sequence_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for positional encoding\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    \n",
        "    # Position encoding matrix\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]   \n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # Apply the cosine to even columns and sin to odds\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    \n",
        "    return pos_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical crossentropy loss function (sparse)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "# Loss tracker \n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class for masked language model\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    \n",
        "    # Function for training step\n",
        "    def train_step(self, inputs):\n",
        "        \n",
        "        # If there are 3 inputs\n",
        "        if len(inputs) == 3:\n",
        "            \n",
        "            # Unpack the inputs\n",
        "            features, labels, sample_weight = inputs\n",
        "        \n",
        "        # If there are 2 inputs\n",
        "        else:\n",
        "            \n",
        "            # Unpack the inputs\n",
        "            features, labels = inputs\n",
        "            \n",
        "            # Set sample weight to None\n",
        "            sample_weight = None\n",
        "\n",
        "        # Tell the model to compute the gradients\n",
        "        with tf.GradientTape() as tape:\n",
        "            \n",
        "            # Get the predictions\n",
        "            predictions = self(features, training=True)\n",
        "            \n",
        "            # Compute the loss\n",
        "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    # Function for listing our `Metric` objects so that `reset_states()` can be called automatically at the start of each epoch or at the start of `evaluate()`. If you don't implement this property, you have to call `reset_states()` yourself at the time of your choosing.\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for creating BERT model for MLM\n",
        "def create_masked_language_bert_model():\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "\n",
        "    # Word embedding layer\n",
        "    word_embeddings = tf.keras.layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\")(inputs)\n",
        "    \n",
        "    # Position embedding layer\n",
        "    position_embeddings = tf.keras.layers.Embedding(input_dim=config.MAX_LEN,\n",
        "                                                    output_dim=config.EMBED_DIM,\n",
        "                                                    weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
        "                                                    name=\"position_embedding\",\n",
        "                                                    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
        "    \n",
        "    # Add the word and position embeddings\n",
        "    embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "    # Get the output of the embedding layer\n",
        "    encoder_output = embeddings\n",
        "    \n",
        "    # Loop over the number of layers\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        \n",
        "        # Create BERT module\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    # Output layer\n",
        "    mlm_output = tf.keras.layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(encoder_output)\n",
        "    \n",
        "    # MLM model\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    # Adam optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    \n",
        "    # Compile the model\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "    \n",
        "    return mlm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Id2Token\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "\n",
        "# Token2Id\n",
        "token2id = {y: x for x, y in id2token.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class for callback function\n",
        "class MaskedTextGenerator(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    # Constructor function\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        \n",
        "        # Initialization\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.k = top_k\n",
        "\n",
        "    # Function for decoding tokens\n",
        "    def decode(self, tokens):\n",
        "        \n",
        "        # Return the decoded tokens\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
        "\n",
        "    # Function for converting ids to tokens\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        \n",
        "        # Return the token\n",
        "        return id2token[id]\n",
        "\n",
        "    # Function to run at the end of each epoch\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        # Make predictions\n",
        "        prediction = self.model.predict(self.sample_tokens)\n",
        "\n",
        "        # Get the masked index\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id) \n",
        "        masked_index = masked_index[1]\n",
        "        \n",
        "        # Get the masked prediction\n",
        "        mask_prediction = prediction[0][masked_index]\n",
        "\n",
        "        # Get the top k predictions\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
        "        \n",
        "        # Get the values\n",
        "        values = mask_prediction[0][top_indices]\n",
        "\n",
        "        # Loop over the top k predictions\n",
        "        for i in range(len(top_indices)):\n",
        "            \n",
        "            # Get the prediction\n",
        "            p = top_indices[i]\n",
        "            \n",
        "            # Get the value\n",
        "            v = values[i]\n",
        "            \n",
        "            # Get the tokens\n",
        "            tokens = np.copy(self.sample_tokens[0])\n",
        "            \n",
        "            # Replace the masked token with the predicted token\n",
        "            tokens[masked_index[0]] = p\n",
        "            \n",
        "            # Results to print\n",
        "            result = {\n",
        "                \"input_text\": self.decode(self.sample_tokens[0].numpy()),\n",
        "                \"prediction\": self.decode(tokens),\n",
        "                \"probability\": v,\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "            }\n",
        "            \n",
        "            # Report\n",
        "            pprint.pprint(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorize the sample text (for callback)\n",
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "\n",
        "# Setup the callback function\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L-UxPIGDPcG"
      },
      "outputs": [],
      "source": [
        "# Create the BERT model for MLM task\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "\n",
        "# Model summary\n",
        "bert_masked_model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vSXpzaMHDPcH"
      },
      "source": [
        "<br>\n",
        "\n",
        "### TRAINING\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV0jlk7MDPcI"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
        "\n",
        "# Save the model\n",
        "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1nlqu7xRDPcI"
      },
      "source": [
        "<br>\n",
        "\n",
        "### FINE-TUNNING\n",
        "\n",
        "-- \n",
        "\n",
        "Fine-tune a sentiment classification model. We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n",
        "To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n",
        "pretrained BERT features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pretrained bert model\n",
        "mlm_model = tf.keras.models.load_model(\"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel})\n",
        "\n",
        "# Construct the pretrained bert model\n",
        "pretrained_bert_model = tf.keras.Model(mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output)\n",
        "\n",
        "# Freeze the layers\n",
        "pretrained_bert_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for creating the classifier model\n",
        "def create_classifier_bert_model():\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "    \n",
        "    # Feed the input to the pretrained bert model\n",
        "    sequence_output = pretrained_bert_model(inputs)\n",
        "    \n",
        "    # Global max pooling\n",
        "    pooled_output = tf.keras.layers.GlobalMaxPooling1D()(sequence_output)\n",
        "    \n",
        "    # Hidden dense layer\n",
        "    hidden_layer = tf.keras.layers.Dense(64, activation=\"relu\")(pooled_output)\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "    \n",
        "    # Construct the mode;\n",
        "    classifer_model = tf.keras.Model(inputs, outputs, name=\"classification\")\n",
        "    \n",
        "    # Adam optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    \n",
        "    # Compile the model\n",
        "    classifer_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    \n",
        "    return classifer_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the classifier model\n",
        "classifer_model = create_classifier_bert_model()\n",
        "\n",
        "# Model summary\n",
        "classifer_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the classifier with frozen BERT stage\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjWX6DtlDPcI"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the BERT model for fine-tuning\n",
        "pretrained_bert_model.trainable = True\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile the model\n",
        "classifer_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model (fine-tuning)\n",
        "classifer_model.fit(train_classifier_ds,\n",
        "                    epochs=5,\n",
        "                    validation_data=test_classifier_ds,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "86qMdXayDPcI"
      },
      "source": [
        "<br>\n",
        "\n",
        "### PREDICTION\n",
        "\n",
        "--- \n",
        "\n",
        "Create an end-to-end model and evaluate it. When you want to deploy a model, it's best if it already includes its preprocessing pipeline, so that you don't have to reimplement the preprocessing logic in your production environment. Let's create an end-to-end model that incorporates the `TextVectorization` layer, and let's evaluate. Our model will accept raw strings\n",
        "as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXFuhVP0DPcI"
      },
      "outputs": [],
      "source": [
        "# Function for end-to-end prediction\n",
        "def get_end_to_end(model):\n",
        "    \n",
        "    # Input layer\n",
        "    inputs_string = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "    \n",
        "    # Vectorize the input\n",
        "    indices = vectorize_layer(inputs_string)\n",
        "    \n",
        "    # Feed to the model\n",
        "    outputs = model(indices)\n",
        "    \n",
        "    # Construct the model\n",
        "    end_to_end_model = tf.keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
        "    \n",
        "    # Adam optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    \n",
        "    # Compile the model\n",
        "    end_to_end_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    \n",
        "    return end_to_end_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the end-to-end model\n",
        "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
        "\n",
        "# TODO: Make prediction\n",
        "sample_text = [\"I have watched this [mask] and it was awesome\"]\n",
        "end_to_end_classification_model.predict(sample_text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "### EVALUATION\n",
        "\n",
        "--- \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the end-to-end model\n",
        "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
        "\n",
        "# Model evaluation\n",
        "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mlm_and_finetune_with_bert",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "d66ae6bff8ed57cee7b655e324e7d941960848ec0080f9fdaeaacf19f5254920"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
